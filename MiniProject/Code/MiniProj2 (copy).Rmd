---
title: "Can machine Learning be used to identify species of Sorbus"
author: "PetraGuy, Imperial College London"
output:
  pdf_document: 
    fig_caption: yes 
header-includes:
  - \usepackage{lineno}
  - \linenumbers
 
---

```{r, echo = FALSE, comment=NA}
date = format(Sys.Date(), "%B %d %Y")
cat(date)
```
```{r}
#This creates pdf from command line, note, sensitive to ' or "

#Rscript -e "library(knitr); knit('MiniProj2.Rmd')"

#Rscript -e "library(rmarkdown); render('MiniProj2.md')"
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=NA)
```


```{r}
#clear the workspace
rm(list = ls())
cat("\014")
#setwd("~/Documents/CMEECourseWork/MiniProject/Code")
library(ggplot2)
library(reshape) # both required for the box plots, otherwise they cant all be presented
                # on one page and therefore difficult to analyse
library(rpart)
library(rpart.plot)# both required for the decision tree
```

```{r}
#Get the data , enter input CSV file name here, for data in data directory
inputfile = 'SorariaCompact1.csv'
Dataname = strsplit(inputfile, "\\.")[[1]][[1]]
fullfile = paste("../Data",inputfile,sep = '/')
Data = read.csv(fullfile)
speciesnames = as.character(unique(unlist(Data$Species))) # uselful for nameing things
numspecies = summary(Data$Species) # useful for comparisons

```

```{r}
#Median imputation 
median_replace1 = function(x){
  ifelse(is.na(x), median(x,na.rm = TRUE), x)
 }

median_replace2 = function(x){
  apply(x,2,median_replace1)
}

Imputed_list = lapply(split.data.frame(Data[,2:12], Data$Species), FUN = median_replace2)
```

```{r}
#The imputed dataframe is a list with species as the elements, the following sticks it back together with a different name so both optiona are available
temp = do.call(rbind, Imputed_list)
Imputed_df = cbind(Data[1], temp)
```

```{r}
# Some algorthms are sensitive to the scale of the data, so here the entire dataframe is scaled
temp1 = (Imputed_df[c(2,3,4,5,9)])/100
temp2 = Imputed_df[c(10,11)]/10
Scaled_df = cbind(Data[1], temp1)
Scaled_df = cbind(Scaled_df, temp2)
Scaled_df = cbind(Scaled_df, Data[c(6,7,8,12)])

# but this might reduce the dissimilarity to much, so this is a semi-scaled datafrane.

temp = Imputed_df[-c(1,6,7,8,12)]
temp = scale(temp)
Semi_Scaled_df = cbind(Imputed_df[c(1,6:8,12)], temp)

```


```{r}
# Model evaluation metrics

accuracy = function(atable){
  a = round(sum(diag(atable)/sum(atable)), digits = 2)
  return(a)
}

precision = function(atable){
  p = vector()
  items = vector()
  no_predictions = dim(atable)[2] 
  for (i in 1:no_predictions){
    items[i] = paste("class",colnames(atable)[i], sep = "_")
    p[i] = round(diag(atable)[i]/(sum((atable)[,i])), digits = 2)
  }
  precisions = cbind(items,p)
  colnames(precisions) = c("Class", "Precision")
  return(precisions)
}

sensitivity = function(atable){
  s = vector()
  no_actuals = dim(atable)[1] 
    for (i in 1:no_actuals){
    s[i] = round(diag(atable)[i]/(sum((atable)[i,])), digits = 2)
  }
  sensitivities = cbind(rownames(atable),s)
  colnames(sensitivities) = c("Species", "Sensitivity")
  return(sensitivities)
}

```


```{r}
#Data sampling and test/train sets.

#This shuffles and splits the data
shuffle = function(dataset){
  splits = list()
  set.seed(42)
  n = nrow(dataset)
  shuffled = dataset[sample(n),]
  train = shuffled[1:round(0.7*n),]
  test = shuffled[(round(0.7*n)+1):n,]
  splits[[1]] = train
  splits[[2]] = test
  return(splits)
}

#this subsets the data into species
create_train_test = function(dataset){
  sets = as.character(unique(dataset[,1]))
  train = data.frame()
  test = data.frame()
  split_data = list()
  for (i in 1:length(sets)){
    sub = subset(dataset, dataset[,1] == sets[i])
    train_temp = shuffle(sub)[[1]]
    test_temp = shuffle(sub)[[2]]
    train = rbind(train, train_temp)
    test = rbind(test, test_temp)
  }
  split_data[[1]] = train
  split_data[[2]] = test
  return(split_data)
}

#PS you can check the splits are correct with summary(train$species), summary(test$species)
#summary(maindata$species), this gives numbers in each species.

#now to include a cross fold validation repeat above fold times

```

```{r}
# performs the k means algorith over 10 repeats, returns BSS/Wss ration, accuracy and 

repeated_kmeans = function(dataset){ 
  metrics_list = list()
  accuracy_vector = vector()
  ratio = vector()
  species_no = data.frame(matrix(ncol = 7))
  colnames(species_no) = speciesnames
  for (i in 1:10){
    kmeans_result = kmeans(dataset[-1], 7, 20, iter.max = 50, algorithm = "MacQueen")
    ratio[i] = round(kmeans_result$tot.withinss/kmeans_result$totss, digits = 2)
    kmeans_conf = table(Imputed_df$Species, kmeans_result$cluster)
    accuracy_vector[i] = accuracy(kmeans_conf)
    species = diag(kmeans_conf)
    species_no = rbind(species_no, species)
  }
 metrics_list[[1]] = ratio
 metrics_list[[2]] = accuracy_vector
 metrics_list[[3]] = species_no[-1,]
 return(metrics_list)
}
```



```{r, message = FALSE }
# data exploration - box plots
melted = melt(Scaled_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
```
```{r, message = FALSE }
# data exploration - box plots
melted = melt(Semi_Scaled_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
```

```{r, message = FALSE }
# data exploration - box plots
melted = melt(Imputed_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
```

```{r}
#Getting the results for the kmeans
#Imputed df without scaling
Imputed_kmeans = repeated_kmeans(Imputed_df) 
#Semi scaled data
Semi_scaled_kmeans = repeated_kmeans(Semi_Scaled_df)
# fully scaled data
Scaled_kmeans = repeated_kmeans(Scaled_df)
```


```{r}
#Disaply BSS/WSS ratio for the kmeans calculated in chunk above 
SS_df = data.frame(nrow = 3)
SS_df = rbind(Imputed_kmeans[[1]],Semi_scaled_kmeans[[1]],Scaled_kmeans[[1]])
rownames(SS_df) = c("unscaled","semi-scaled","fullyscaled")
colnames(SS_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
SS_df

```


```{r}
#Display accuaracy for kmeans calcualted above
prec_df = data.frame(nrow = 3)
prec_df = rbind(Imputed_kmeans[[2]],Semi_scaled_kmeans[[2]],Scaled_kmeans[[2]])
rownames(prec_df) = c("unscaled","semi-scaled","fullyscaled")
colnames(prec_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
prec_df
```



```{r}
#Display percentage of true positives from the confusion matrix calcualted in kmeans chunk above
m1 = Imputed_kmeans[[3]]
m2 = Semi_scaled_kmeans[[3]]
m3 = Scaled_kmeans[[3]]


m1_percent = round(apply(m1, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m1_percent) = c(1:10)
cat("unscaled")
m1_percent
cat("\n")

m2_percent = round(apply(m2, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m2_percent) = c(1:10)
cat("semi-scaled")
m2_percent
cat("\n")

m3_percent = round(apply(m3, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m3_percent) = c(1:10)
cat("scaled")
m3_percent
cat("\n")

```



```{r}
repeated_hclust = function(dataset){
  conf = list()
  metrics_list = list()
  accuracy_vector = vector()
  dist_methods = c("euclidean", "maximum","manhattan","canberra","minkowski")
for (i in 1:length(dist_methods)){
  method = dist_methods[i]
  distance = dist(dataset[-1], method = method)
  hcluster = hclust(distance, method = "complete")
  cluster = cutree(hcluster, k = 7)
  conf[[i]] = table(dataset$Species, cluster)
  accuracy_vector[i] = accuracy(conf[[i]])
}
  accuracy_df = rbind(dist_methods, accuracy_vector)
  metrics_list[[1]]=accuracy_df
  metrics_list[[2]]=conf
  names(metrics_list)=c("Accuracy", "Confusion Matrix")
  return(metrics_list)

}

hcluster = repeated_hclust(Imputed_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster[[1]][1,], hcluster[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
cat("Accuracy obtained using the different distance calculations\n")
print(accs)
```


```{r}
hcluster[[2]][[4]]
```

Decision Tree

```{r}

Imputed_sets = create_train_test(Imputed_df)
Imputed_train = Imputed_sets[[1]]
Imputed_test = Imputed_sets[[2]]
tree = rpart(Species~., Imputed_train, method = "class", control = rpart.control(cp = 0.00001))
rpart.plot(tree, box.palette = "Blues")
```

The decision tree produces some very accurate nodes. 100 % for S. minima, 90% for S. anglica, 

```{r}
pred_tree = predict(tree, Imputed_test, type = "class")
confusion_tree = table(Imputed_test$Species, pred_tree)
cat("The accuracy for the decision tree is ")
cat(accuracy(table(Imputed_test$Species, pred_tree)))
```

```{r}
cat("The sensitivies for the decision tree model are shown in the table below \n")
(sensitivity(table(Imputed_test$Species, pred_tree)))
```

```{r}
cat(" The precisions for the decision tree model are shown in the table below \n")
precision(table(Imputed_test$Species, pred_tree))

```

```{r}
cat("The confusion matrix details exactly how the species were placed. \n")
cat("The final column is the sum of species in the test set. \n")
cbind(table(Imputed_test$Species, pred_tree),summary(Imputed_test$Species))
```







