---
output:
  pdf_document: 
    fig_caption: yes
    df_print: kable
---
---
title: "Using machine learning be used to identify species of Sorbus"
author: "PetraGuy"
date: "6 January 2018"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r, echo = FALSE}
#clear the workspace
rm(list = ls())
cat("\014")
setwd("~/Documents/CMEECourseWork/MiniProject/Code")
#library(randomForest)
#library(ggplot2)
#library(gridExtra)
#library(GGally)
#library(grid)
#library(reshape2)
#library(caret)
#library(rpart)
#library(rattle) this will not install
#library(rpart.plot)
#library(RColorBrewer)
```
Abstract.

Machine learning was used to separate six species of Sorbus within the subgenus Soraria based on morphological measurements of fruit and leaves....

1. Introduction - The genus Sorbus.

Sorbus is a member of the Rosacea family, perhaps the best known species being Sorbus aucuparia, the Rowan or Mountain Ash. 

![Figure 1. Sorbus aucuparia](Sorbusaucuparia.pdf){width=25%, height=25%}\ 


However, there are over 50 species of Sorbus in the UK, 38 of these are vulnerable or critically endangered and most are endemic or native.  There are four diploid species, but, as with many Rosaceae, Sorbus produce new apomictic polyploid species. These can also produce viable pollen and can therefore backcross with other diploid species. This results in the large number of genetically unique, stable, clonal communities, which can look very similar to each other. This presents a problem with recording and many Sorbus require expert knowledge to correctly identify to species level because much of the identification depends on comparitive knowledge. This tends to disuade recorders, or encourages records at aggregate level. 

Sorbus are grouped into six subgenus, each of which are reasonably easy to identify by recorders with some knowledge, as the illustrations below show.

More difficulty arises when identifying plants within these subgeni, and this is where this work has concentrated. In this modelling only the subgenus Soraria has been trialled. This consists of seven species all similar in appearance to Sorbus intermedia, although only six species are considered based on the availability of data. These plants are distinguished from other subgeni by having leaves with rounded lobes which are tomentose beneath and the fruits having fewer lenticles. Perhaps the most noticeable difference between plants within the subgenus, are the larger fruits on S intermedia, the smaller leaves of S minima and the small fruits of S mougeotii.

2. Methods
2.1 Data.

The data was provided by Dr T Rich, the British Botanical Society expert on Sorbus and  consists of leaf and fruit measurements. For the leaves, the length, width, base angle, number of veins, depth of the lobes, vein angle and base angle have been recorded. For the fruit, the length and the width are used. Due to the variability in leaf size across one plant, the measurements were all carried out in a specific manner described by Rich [ ]. Essentially, repeated measurements of leaves on sterile spurs on the sunlight side of the tree are recorded and averaged over at least ten leaves. 

The nature of collection means that the data was sparse. Every plant of every species did not have have complete set of measurements or the same number of measurements. For example, S intermedia had 126 observations but S leyana only had 39. This is due to the rarity S. leyana. S. intermedia is a common plant found throught the UK in easily accessible places, whilst S leyana is only found in two sites in South Wales, sometimes on the sides of cliffs. In addition,  measurements  cannot all be collected at the same time. Leaves must be measured when mature, around flowering time, and therefore cannot be measured in conjunction with fruit. Separate trips to re-measure fruit on the same trees may not be possible. This has lead to a sparse dataset in which not all morphological characteristics were availabe for every plant. S intermedia records are an example. Of 122 records,  72 are purely for fruit measurements and the remaining 50 purely for leaf measurements, and these occur on different plants If imputation was carried out, 59% of the leaf measurements would be imputed. This would reduce the effectiveness of some algorithsms. For example, in kNN, if you increase the frequency of the neighbours in the S. Intermedia group, it is more likely that a member of a different group will be close to that neighbour. Therefore, the sparsity was handled by reallocating measurements. For example, the 50 leaf measurements for S. intermedia were assigned to 50 fruit measurements and the excess 22 were not used. In some cases, where there were only a few additional rows of incomplete data, median imputation was carried out. 

Although it seems dubious to assign records from one plant to another, in this analysis this was felt to be acceptible for two reasons. Firstly, this an exploration of a new technique for biological recording. It is not currently being proposed as a complete and accurate method for species identifiction at this stage. Secondly, the clonal nature of these plants implies that we would expect a great deal of similarity within a species. The variation within the species is more likely to come from the variety of leaf sizes which can be found on one plant, and these are controlled for, although they cannot be eliminated, when the data is collected. (The range of leaf sizes within each plant was not available, so this comparison has not been tested).

```{r,echo = FALSE }
#Get the data , enter input CSV file name here, for data in data directory
inputfile = 'SorariaCompact1.csv'
Dataname = strsplit(inputfile, "\\.")[[1]][[1]]
fullfile = paste("../Data",inputfile,sep = '/')
Data = read.csv(fullfile)
```


```{r,echo = FALSE}
#Do some median imputations, carried out after the data was manipulated as described above - #hence reducing the number or rows that were imputed. 
median_replace1 = function(x){
  ifelse(is.na(x), median(x,na.rm = TRUE), x)
 }

median_replace2 = function(x){
  apply(x,2,median_replace1)
}

Imputed_list = lapply(split.data.frame(Data[,2:12], Data$Species), FUN = median_replace2)
```



```{r, echo = FALSE}
#The imputed dataframe is a list with species as the elements, the following sticks it back together with a different name so both optiona are available
temp = do.call(rbind, Imputed_list)
Imputed_df = cbind(Data[1], temp)
```


```{r,echo = FALSE }
#Its worth creating a scaled dataframe too for some of the analysis and ploting
Scaled_df = scale(Imputed_df[-1])
Scaled_df = cbind(Data[1], Scaled_df)

# Scaled_df is sceled, imputed df, but if we want to use non-imputed, we need to scale Data again
```

```{r,echo = FALSE }
#scale the non-imputed data, not sure i need this
Scaled_df_NI = scale(Data[-1])
Scaled_df_NI = cbind(Data[1], Scaled_df_NI)

```

3.Data exploration.
In order for machine learning algorithms to work accurately the groups should be separated into clearly defined clumps with very little overlap. The following exploration looks at the way the varibles are distributed.  

The data contains many variables, some of which may be unnecessary to the analysis. The following plots look at correlations, although, for the machine learning algorithms used, this may not be an issue. In fact, dropping variables could cause data points to become closer together and therefore grouped together by a clustering algorithm, when in fact they were not related. But also, having many variables does introduce more neighbours, so there is the potential for for datapoints in separate groups to be close to a un-related neighbour simply because there are more variables and therefore more points.


```{r,echo = FALSE }
#Execute to save the data, or go straight to plotting to view graphs in document
out = paste(Dataname, "pairs", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.pdf')

```

Pairs Plots,  Not all pairs plotted; there are so many the plot becomes too difficult to read and therefore uninformative. 

```{r, echo = FALSE }
#unhash pdf to save data to file.
#pdf(out)
reduced_df = Scaled_df[-c(4,5,6,7,8,9,12)]
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(reduced_df[-1], lower.panel = panel.smooth, upper.panel = panel.cor, col = Scaled_df$Species)
mtext(Dataname, side = 1)
```

Repeat the pairs plots using ggplot

```{r,echo = FALSE }

ggpairs(data = Scaled_df[,2:6])

ggpairs(Scaled_df[,7:12])
```
The pairs plot show a strong correlation between leaf width and length, leaf width andlength and the widest point,  and fruit width and length.

Look at scatter plots by species of main characters.
```{r, echo = FALSE}
#run the following to get new filename if saving data
out = paste(Dataname, "scatter", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.pdf')

```

Scatter plots of the most correlated features

```{r,echo = FALSE }

#unhash pdf to save data to file
#pdf(out)
graphics.off()
reduced_df = Imputed_df[Imputed_df$Species == "Arranensis",]
ggplot(reduced_df,aes(x= LeafLength, y = LeafWidth)) +
                 geom_point()
plot1= ggplot(Imputed_df , aes(x= LeafLength, y = LeafWidth, col = Species))+
  geom_point()
plot2 = ggplot(Imputed_df, aes(x= FruitLength, y = FruitWidth, col = Species))+
  geom_point()

grid.arrange(plot1,plot2)
```
The scatter plots do not demostrate a clear separation of features between species, there is considerable overlap. S. anglica appears to have some separation with the longest and widest leaves, but the range of leaf measurements plotted shows that it overlaps witj most others, perhaps excepting S. mougeotii. S.Anglica stands out in the second plot as having fruits wider than long. This is further demonstrated by the box plots below.

Box plots
```{r,echo = FALSE }
out = paste(Dataname, "box", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.pdf')
```


```{r,echo = FALSE }
#pdf(out)
library(ggplot2)
library(grid)
col_names = colnames(Data)
col_names = col_names[-1]
plotlist = list()

for (i in seq_along(col_names)){
  plot = ggplot(Imputed_df, (aes_string(x='Species', y=col_names[i])))+
                 geom_boxplot()
  plot(plot)
  #plotlist = rbind(plotlist,  plot)
  #grid.draw(plot)
}

```

Try ggplot option for boxplots, but they'll need scaling because facet wrap will use same scale for all

```{r,echo = FALSE }
out = paste(Dataname, "scaledbox", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.pdf')
```


```{r,echo = FALSE, message = FALSE }
#pdf(out)
library(reshape)
melted = melt(Scaled_df)

ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
```
Box plots clearly show a lot of  overlap for different features across the species. Some  features clearly separate one or two species from the others. For example the fruit length differentiates S. leyana and S. mougeotti and the fruit width for S.anglica does not overlap with other species. There is also considerable overlap of many features and no one feature obviously separates all the species. This suggests that the machine learning algorithms that are using distance techniques, such as k means and k nearest neighbours may not differentiate groups, but decision tree methds, such as random forest, may be better.



=====Not sure that I wan to include this bit =======
Analysis of variance for each feature quantifies this.
```{r,echo = FALSE }
out = paste(Dataname, "anova", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.csv')

```


```{r,echo = FALSE }

col_names = colnames(Data)
col_names = col_names[-1]
anova_data = data.frame()

for (i in seq_along(col_names)){
    m = lm(formula = paste(col_names[i],"~Species"), data =  Imputed_df)
    ANOVA <- anova(m)
    r2 <- summary(m)$r.squared
    data = data.frame(col_names[i], ANOVA[[4]][1], ANOVA[[5]][1], r2)
    anova_data = rbind(anova_data,data)
    
    
  }
names(anova_data) = c("Feature","F", "p", "r2")
print(anova_data)
write.csv(anova_data, out)
```

But what happens if we need to scale the data, repeated anovas for scaled dataframe

```{r, echo = FALSE}
out = paste(Dataname, "anova_scaled", sep = "_")
out= paste('../Results',out,sep = '/')
out = paste(out,'.csv')
```

```{r,echo = FALSE }
col_names = colnames(Data)
col_names = col_names[-1]
anova_data = data.frame()

for (i in seq_along(col_names)){
    m = lm(formula = paste(col_names[i],"~Species"), data =  Scaled_df)
    ANOVA <- anova(m)
    r2 <- summary(m)$r.squared
    data = data.frame(col_names[i], ANOVA[[4]][1], ANOVA[[5]][1], r2)
    anova_data = rbind(anova_data,data)
    
    
  }
names(anova_data) = c("Feature","F", "p", "r2")
print(anova_data)
write.csv(anova_data, out)
```

There is a difference with within group means and between group means, as we can see from the large F values, except in WidestPercent. LeafWidth, LeafLength and WidestPoint are highly correlated and may not all be requried in a model


====== delete all above ??? =====



A simple k-means cluster analysis can be used to give an indiction of the groupings.

```{r,echo = FALSE }
set.seed(1)

graphics.off()

Imputed_kmeans = kmeans(Imputed_df[-1], 7)
table(Imputed_df$Species, Imputed_kmeans$cluster)

plot1= ggplot(Imputed_df, aes(x= LeafLength, y = LeafWidth, col = Imputed_kmeans$cluster))+
  geom_point()

plot2= ggplot(Imputed_df, aes(x= LeafLength, y = LeafWidth, col = Species))+
  geom_point()

grid.arrange(plot1,plot2)
```
The first plot shows the modelled clusters and the second is the scatterplot of the real data for comparison. As we suspected, S. anglica has been allocated to most clusters, S. cuniefolia and S. mougeotti are the most differentiated. 


==== May not include any of this =======


What about principle components?

```{r, echo = FALSE}
pc = princomp(Scaled_df[-1])
plot(pc)
plot(pc, type = 'l')

```

```{r, echo = FALSE}
pc = prcomp(Scaled_df[-1])
comp = data.frame(pc$x[,1:4])
plot(comp)
loadings = eigen(cov(Scaled_df[-1]))$vectors
explvar = round((loadings^2),digits = 2)
rownames(explvar) =  col_names
print(explvar)

```
If leaf length is correlated with width and widest point and leaf width, AND pc shows that leafwidth, veins, leaf ratio, fruit length and widest percent together explain 80% of the variance, perhaps a model using only these is sufficient.

```{r, echo = FALSE}
reduced_data = Imputed_df[-c(1,2,4,5,8,11,12)]
Imputed_kmeans = kmeans(reduced_data, 7, nstart = 7)
table(Imputed_df$Species, Imputed_kmeans$cluster)
```
Again, S. cuneifolia, S mougeotti are differentiated, there does not appear to be much improvement in the other species.


=== Delete all above ========



Modelling. Supervised Learning.

Supervised machine learning is appropriate because we have pre-defined target variables into which we want to split the data - the species. Three classification models will be used, kmeans, k nearest neighbouts and random forest. The caret package was used for all the modelling. In order to reduce overfitting the caret package allows for train and test sets to be easily created within the code. Multiple train sets are created for each model by splitting the entire data set several times and fitting the model to each split.Every point in the data will fall into on training set fold, and therefore the influence of outliers is reduced. The folds are used to estimate the out of sample error and the enitre data set can then be used to refit the model.

The first model is a random forest.The caret packages was used first. This provides straight forward cross validated mode, in which the data is split into n different test and train sections. The tune length(number of possible variables to use at each decision node in the tree can be set - mtry) and the model outputs the accuracy for all the possible values of mtry. This means we can easily select a value for the mtry hyperparameter.

```{r, echo = FALSE}

set.seed(42)
soraria_tree_caret_ranger = train(Species~., data = Imputed_df, method = "ranger", tuneLength =11,
               trControl= trainControl(method = "cv", number = 5, summaryFunction = defaultSummary, classProbs = TRUE))
plot(soraria_tree_caret_ranger)
print(soraria_tree_caret_ranger)

                


```

The plot shows that 6 parameters give the most acurate model using the extratrees algorithm for each node.

The randomForest packages provides simple way of accessing model and outputting the errors. The randomForest command bootstraps samples to create a forest and averages the error over the collection. Since not all items are selected in each sample there will be a set of unused variables that can be used to calcualte the error of the model. This is called the out of bag error and the error against the number of trees used can be plotted in order to estimate how many tees should be used. This package also provides a confusion matrix which easily shows the results of the model, something the caret model did not. Unfortuneately the out of bag samples are not available as an object, so tuning the model required the creation of a separate test and train set. 

```{r}
#using random forest package
#create the train test split to be used for model tuning.
n = nrow(Imputed_df)
n_train = round(0.8*n)
set.seed(42) # change the seed to create new train and test sets
train_indices = sample(1:n, n_train)
soraria_train = Imputed_df[train_indices,]
soraria_test = Imputed_df[-train_indices,]
```

```{r}
soraria_randomforest = randomForest(formula = Species~., data = Imputed_df, mtry = 6, keep.forest = FALSE)
err = soraria_randomforest$err.rate
plot(soraria_randomforest)
legend(x = "right", legend = colnames(err), fill = 1:ncol(err))
```

```{r}
#tuning the mtry
res = tuneRF(x = soraria_train,
             y = soraria_train$Species,
             ntreeTry = 500) 
print(res)
mtry_opt = res[,"mtry"][which.min(res[,"OOBError"])]
```
We can tune for more hyperparameters by minimizing the OOB error.
above we tuned mtry, but we can also look at nodesize and sampsize. 
Nodesize is the minimum size of the terminal nodes and will decide how deep the tree is. The larger this number, the smaller the tree. 

```{r}
set.seed(43)
soraria_rpart_tree = rpart(Species~., soraria_train, method = "class", parms = list(split = "gini"))
rpart.plot(soraria_rpart_tree)
```
Gives  great plot, but there's not much tuning to be done
Look at accuracy

```{r}
pred = predict(soraria_rpart_tree, soraria_test, type = "class")
conf = table(soraria_test$Species, pred)
print(sum(diag(conf))/sum(conf))
print(conf)
```


